{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGXSa1SzwJKZv4f8Ir3cBa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Robert97-2/assignment/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROBERT OMONDIIN14/00055/21\n",
        "SAIMON OKINYI IN14/00046/21\n",
        "JOSHUA OCHIENG IN14/00056/21"
      ],
      "metadata": {
        "id": "XJlj-kKISkOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building character-level language models in\n",
        "Keras"
      ],
      "metadata": {
        "id": "b9icnudAS3YA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qJPb432P7WA",
        "outputId": "12bdfd03-b52b-4617-aa01-dbdb902eb976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Bidirectional, Dropout, SimpleRNN, GRU, BatchNormalization\n",
        "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "from tensorflow.keras.utils import get_file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading shakespeare's halmet"
      ],
      "metadata": {
        "id": "CFJ901YfSRpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "hamlet = gutenberg.words('shakespeare-hamlet.txt')\n",
        "text = ''\n",
        "for word in hamlet:  # For each word\n",
        "    text += str(word).lower()  # Convert to lower case and add to string\n",
        "    text += ' '  # Add space\n",
        "print('Corpus length, Hamlet only:', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIL54TaDQ3_H",
        "outputId": "317bf9de-ce97-4c42-a7b4-ca92884efc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length, Hamlet only: 166765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "building dictionary of characters"
      ],
      "metadata": {
        "id": "ZKDa_uqcSKjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(text)))\n",
        "print('Total characters:', len(characters))\n",
        "char_indices = dict((l, i) for i, l in enumerate(characters))\n",
        "indices_char = dict((i, l) for i, l in enumerate(characters))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "719xp2RERNOC",
        "outputId": "b8a42b52-4325-4d30-d68e-361a7d319dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARING TRAINING SEQUENCES OF CHARACTERS"
      ],
      "metadata": {
        "id": "-_oDtWCkS-EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Break text into features and labels\n",
        "training_sequences = []  # Empty list to collect each sequence\n",
        "next_chars = []  # Empty list to collect next character in sequence\n",
        "seq_len, stride = 35, 1  # Define length of each input sequence & stride to move before sampling next sequence\n",
        "\n",
        "for i in range(0, len(text) - seq_len, stride):  # Loop over text with window of 35 characters, moving 1 stride at a time\n",
        "    training_sequences.append(text[i: i + seq_len])  # Append sequences to training_sequences\n",
        "    next_chars.append(text[i + seq_len])  # Append following character in sequence to next_chars\n",
        "\n",
        "print('Number of training sequences:', len(training_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Cmh6wjTIbe",
        "outputId": "a990598a-5d2c-49a6-e526-474a4c0a0dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sequences: 166730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing out example sequences"
      ],
      "metadata": {
        "id": "0vs1suYdTduV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out sequences and labels to verify\n",
        "print('Number of sequences:', len(training_sequences))\n",
        "print('First sequences:', training_sequences[:1])\n",
        "print('Next characters in sequence:', next_chars[:1])\n",
        "print('Second sequences:', training_sequences[1:2])\n",
        "print('Next characters in sequence:', next_chars[1:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3xhmCViTkXl",
        "outputId": "98da9514-ccf8-4c9d-e2fc-11b5577c89c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 166730\n",
            "First sequences: ['[ the tragedie of hamlet by william']\n",
            "Next characters in sequence: [' ']\n",
            "Second sequences: [' the tragedie of hamlet by william ']\n",
            "Next characters in sequence: ['s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectoring training data"
      ],
      "metadata": {
        "id": "HM9lEUK9TyRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Matrix of zeros\n",
        "# With dimensions: (training sequences, length of each sequence, total unique characters)\n",
        "x = np.zeros((len(training_sequences), seq_len, len(characters)), dtype=np.bool_)\n",
        "y = np.zeros((len(training_sequences), len(characters)), dtype=np.bool_)\n",
        "\n",
        "for index, sequence in enumerate(training_sequences):  # Iterate over training sequences\n",
        "    for sub_index, char in enumerate(sequence):  # Iterate over characters per sequence\n",
        "        x[index, sub_index, char_indices[char]] = 1  # Update character position in feature matrix to 1\n",
        "    y[index, char_indices[next_chars[index]]] = 1  # Update character position in label matrix to 1\n",
        "\n",
        "print('Data vectorization completed.')\n",
        "print('Feature vectors shape', x.shape)\n",
        "print('Label vectors shape', y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NSLV8cIT1Lh",
        "outputId": "070be412-d63a-4025-dd2e-5656ce4bc901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data vectorization completed.\n",
            "Feature vectors shape (166730, 35, 43)\n",
            "Label vectors shape (166730, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stochastic sampling\n"
      ],
      "metadata": {
        "id": "dc3uGElsfP8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(softmax_predictions, sample_threshold=1.0):\n",
        "    softmax_preds = np.asarray(softmax_predictions).astype('float64')\n",
        "    # Make array of predictions, convert to float\n",
        "    log_preds = np.log(softmax_preds) / sample_threshold\n",
        "    # Log normalize and divide by threshold\n",
        "    exp_preds = np.exp(log_preds)\n",
        "    # Compute exponents of log normalized terms\n",
        "    norm_preds = exp_preds / np.sum(exp_preds)\n",
        "    # Normalize predictions\n",
        "    prob = np.random.multinomial(1, norm_preds, 1)\n",
        "    # Draw sample from multinomial distribution\n",
        "    return np.argmax(prob)\n",
        "    #Return max value\n"
      ],
      "metadata": {
        "id": "LhEwIo_mfVi2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "use custom callbacks to genarate text\n"
      ],
      "metadata": {
        "id": "gHUiL3kcftJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    global model, model_name # Indented this line to be part of the function\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    start_index = random.randint(0, len(text) - seq_len - 1)\n",
        "    # Random index position to start sample input sequence\n",
        "    end_index = start_index + seq_len\n",
        "    # End of sequence, corresponding to training sequence length\n",
        "    sampling_range = [0.3, 0.5, 0.7, 1.0, 1.2]\n",
        "    # Sampling entropy threshold\n",
        "    for threshold in sampling_range:\n",
        "        print('----- *Sampling Threshold* :', threshold)\n",
        "        generated = ''\n",
        "        # Empty string to collect sequence\n",
        "        sentence = text[start_index: end_index]\n",
        "        # Random input sequence taken from Hamlet\n",
        "        generated += sentence\n",
        "        # Add input sentence to generated\n",
        "        print('Input sequence to generate from : \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "        # Print out buffer instead of waiting till the end\n",
        "        for i in range(400):\n",
        "            # Generate 400 next characters in the sequence\n",
        "            x_pred = np.zeros((1, seq_len, len(characters)))\n",
        "            # Matrix of zeros for input sentence\n",
        "            for n, char in enumerate(sentence):\n",
        "                # For character in sentence\n",
        "                x_pred[0, n, char_indices[char]] = 1.\n",
        "                # Change index position for character to 1.\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            # Make prediction on input vector\n",
        "            next_index = sample(preds, threshold)\n",
        "            # Get index position of next character using sample function\n",
        "            next_char = indices_char[next_index]\n",
        "            # Get next character using index\n",
        "            generated += next_char\n",
        "            # Add generated character to sequence\n",
        "            sentence = sentence[1:] + next_char\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()"
      ],
      "metadata": {
        "id": "ayOJSAYwf0yh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing multiple models"
      ],
      "metadata": {
        "id": "4kIB7wx5g7q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_models(list, epochs=10):\n",
        " global model, model_name\n",
        " for network in list:\n",
        " print('Initiating compilation...')\n",
        " # Initialize model\n",
        " model = network()\n",
        " # Get model name\n",
        " model_name = re.split(' ', str(network))[1]\n",
        "def test_models(list, epochs=10):\n",
        " global model, model_name\n",
        " for network in list:\n",
        "  print('Initiating compilation...')\n",
        "  # Initialize model\n",
        "  model = network()\n",
        "  # Get model name\n",
        "  model_name = re.split(' ', str(network))[1]\n",
        "  #Filepath to save model with name, epoch and loss\n",
        "  filepath = \"C:/Users/npurk/Desktop/Ch5RNN/all_models/versions/%s_epoch-{epoch:02d}-loss-{loss:.4f}.h5\"%model_name\n",
        "  #Checkpoint callback object\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
        "  # Compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  print('Compiled:', str(model_name))\n",
        "  # Initiate training\n",
        "  network = model.fit(x, y, batch_size=100, epochs=epochs, callbacks=[print_callback, checkpoint])\n",
        "  # Print model configuration\n",
        "  model.summary()\n",
        "  #Save model history object for later analysis\n",
        "  with open('C:/Users/npurk/Desktop/Ch5RNN/all_models/history/%s.pkl'%model_name, 'wb') as file_pi:\n",
        "   pickle.dump(network.history, file_pi)\n",
        " #Checkpoint callback object\n",
        " checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0,\n",
        "save_best_only=True, mode='min')\n",
        " # Compile model\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        " print('Compiled:', str(model_name))\n",
        " # Initiate training\n",
        " network = model.fit(x, y,\n",
        " batch_size=100,\n",
        " epochs=epochs,\n",
        " callbacks=[print_callback, checkpoint])\n",
        " # Print model configuration\n",
        " model.summary()\n",
        " #Save model history object for later analysis\n",
        " with\n",
        "open('C:/Users/npurk/Desktop/Ch5RNN/all_models/history/%s.pkl'%model_name,\n",
        "'wb') as file_pi:\n",
        " pickle.dump(network.history, file_pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Vt4PXadhg-UI",
        "outputId": "fae01fc6-0a46-43d8-f86b-1712e6a499da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 11) (<ipython-input-5-0e8709115530>, line 11)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-0e8709115530>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    \"C:/Users/npurk/Desktop/Ch5RNN/all_models/versions/%s_epoch-{epoch:02d}-Recurrent Neural Networks Chapter 5\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_models(list, epochs=10):\n",
        "    global model, model_name\n",
        "    for network in list:\n",
        "        print('Initiating compilation...')\n",
        "        # Initialize model\n",
        "        model = network()\n",
        "        # Get model name\n",
        "        model_name = re.split(' ', str(network))[1]\n",
        "        #Filepath to save model with name, epoch and loss\n",
        "        filepath = \"C:/Users/npurk/Desktop/Ch5RNN/all_models/versions/%s_epoch-{epoch:02d}-loss-{loss:.4f}.h5\"%model_name\n",
        "        #Checkpoint callback object\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
        "        # Compile model\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "        print('Compiled:', str(model_name))\n",
        "        # Initiate training\n",
        "        network = model.fit(x, y, batch_size=100, epochs=epochs, callbacks=[print_callback, checkpoint]) # Assuming print_callback is defined elsewhere\n",
        "        # Print model configuration\n",
        "        model.summary()\n",
        "        #Save model history object for later analysis\n",
        "        with open('C:/Users/npurk/Desktop/Ch5RNN/all_models/history/%s.pkl'%model_name, 'wb') as file_pi:\n",
        "            pickle.dump(network.history, file_pi)"
      ],
      "metadata": {
        "id": "BENmjnqCheOL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "building a simple RNN"
      ],
      "metadata": {
        "id": "J6UQaJcnh01i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Bidirectional, Dropout\n",
        "from keras.layers import SimpleRNN, GRU, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "'''Fun part: Construct a bunch of functions returning different kinds of\n",
        "RNNs, from simple to more complex'''\n",
        "def SimpleRNN_stacked_model():\n",
        " model = Sequential()\n",
        " model.add(SimpleRNN(128, input_shape=(seq_len, len(characters)),\n",
        "return_sequences=True))\n",
        " model.add(SimpleRNN(128))\n",
        " model.add(Dense(len(characters), activation='softmax'))\n",
        " return model\n"
      ],
      "metadata": {
        "id": "Dpwhv_-yh3eA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STACKING RNN LAYERS"
      ],
      "metadata": {
        "id": "X0DdOd9-iJNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SimpleRNN_stacked_model():\n",
        " model = Sequential()\n",
        " model.add(SimpleRNN(128, input_shape=(seq_len, len(characters)),\n",
        "return_sequences=True))\n",
        " model.add(SimpleRNN(128))\n",
        " model.add(Dense(len(characters), activation='softmax'))\n",
        " return model"
      ],
      "metadata": {
        "id": "O9qztWvtiMID"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "building GRUs"
      ],
      "metadata": {
        "id": "las1vXgQiVp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_stacked_model():\n",
        " model = Sequential()\n",
        " model.add(GRU(128, input_shape=(seq_len, len(characters)),\n",
        "return_sequences=True))\n",
        " model.add(GRU(128))\n",
        " model.add(Dense(len(characters), activation='softmax'))\n",
        " return model"
      ],
      "metadata": {
        "id": "uZPBpH7giY3X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bi direction layer in keras"
      ],
      "metadata": {
        "id": "Z6ZwPhxgi85d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Bi_directional_GRU():\n",
        " model = Sequential()\n",
        " model.add(Bidirectional(GRU(128, return_sequences=True),\n",
        "input_shape=(seq_len, len(characters))))\n",
        " model.add(Bidirectional(GRU(128)))\n",
        " model.add(Dense(len(characters), activation='softmax'))\n",
        " return model"
      ],
      "metadata": {
        "id": "TZmgKQgyjA4_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing recurrent dropout"
      ],
      "metadata": {
        "id": "naf_4OKajF3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Bidirectional, Dropout\n",
        "from keras.layers import SimpleRNN, GRU, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "'''Fun part: Construct a bunch of functions returning different kinds of\n",
        "RNNs, from simple to more complex'''\n",
        "def SimpleRNN_model():\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(128, input_shape=(seq_len, len(characters))))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def SimpleRNN_stacked_model():\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(128, input_shape=(seq_len, len(characters)),\n",
        "                       return_sequences=True))\n",
        "    model.add(SimpleRNN(128))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def GRU_stacked_model():\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape=(seq_len, len(characters)),\n",
        "                   return_sequences=True))\n",
        "    model.add(GRU(128))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def Bi_directional_GRU():\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True),\n",
        "                            input_shape=(seq_len, len(characters))))\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def larger_GRU():\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape=(seq_len, len(characters)),\n",
        "                   dropout=0.2,\n",
        "                   recurrent_dropout=0.2,\n",
        "                   return_sequences=True))\n",
        "    model.add(GRU(128, dropout=0.2,\n",
        "                   recurrent_dropout=0.2,\n",
        "                   return_sequences=True))\n",
        "    model.add(GRU(128, dropout=0.2,\n",
        "                   recurrent_dropout=0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "# All defined models\n",
        "all_models = [SimpleRNN_model, #Make sure that you have defined `SimpleRNN_model` in your notebook before using it\n",
        "              SimpleRNN_stacked_model,\n",
        "              GRU_stacked_model,\n",
        "              Bi_directional_GRU,\n",
        "              Bi_directional_GRU,\n",
        "              larger_GRU]"
      ],
      "metadata": {
        "id": "dNzvrx_1jZNL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing output values"
      ],
      "metadata": {
        "id": "OB1h7Il0kij4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # ... (your existing code) ...\n",
        "\n",
        "    # Add visualization here\n",
        "    plt.figure(figsize=(10, 5))  # Adjust figure size as needed\n",
        "    plt.plot(preds)  # Plot the raw output values\n",
        "    plt.title(f\"Model Output Values (Epoch {epoch}, Threshold {threshold})\")\n",
        "    plt.xlabel(\"Character Index\")\n",
        "    plt.ylabel(\"Output Value\")\n",
        "    plt.show()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "iB0DiDyNkjka"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the output of heavier GRU models\n"
      ],
      "metadata": {
        "id": "kQswilG4mHH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-vis --upgrade\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "import tensorflow as tf\n",
        "# Import Iterable from collections.abc\n",
        "from collections.abc import Iterable  # Changed import statement here\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Bidirectional, Dropout, SimpleRNN, GRU, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "from vis.visualization import visualize_activation\n",
        "from vis.utils import utils\n",
        "\n",
        "# ... (Your existing code for data loading, preprocessing, and model definitions) ...\n",
        "\n",
        "\n",
        "def visualize_gru_activations(model, input_sequence):\n",
        "    \"\"\"\n",
        "    Visualizes the activations of GRU layers in the model.\n",
        "\n",
        "    Args:\n",
        "        model: The Keras model to visualize.\n",
        "        input_sequence: The input sequence to feed to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the GRU layers in the model\n",
        "    gru_layers = [layer for layer in model.layers if isinstance(layer, GRU)]\n",
        "\n",
        "    # Create a new model with outputs from the GRU layers\n",
        "    layer_outputs = [layer.output for layer in gru_layers]\n",
        "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "    # Get the activations for the input sequence\n",
        "    activations = activation_model.predict(input_sequence)\n",
        "\n",
        "    # Plot the activations for each GRU layer\n",
        "    for i, layer_activation in enumerate(activations):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(layer_activation[0], cmap='viridis', aspect='auto')\n",
        "        plt.title(f\"GRU Layer {i + 1} Activations\")\n",
        "        plt.ylabel(\"Hidden Units\")\n",
        "        plt.xlabel(\"Time Steps\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "# ... (Your existing code for training and evaluation) ...\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'model' is your trained GRU model and 'x' is your input data\n",
        "# Select a random input sequence for visualization\n",
        "random_index = np.random.randint(0, len(x))\n",
        "input_sequence = x[random_index:random_index + 1] # Reshape to (1, seq_len, features)\n",
        "\n",
        "# Visualize activations\n",
        "visualize_gru_activations(model, input_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LhME7UDzmIBJ",
        "outputId": "1e6fd430-af0b-4217-836e-00e22f5afef2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-vis in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.12.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from keras-vis) (0.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from keras-vis) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py->keras-vis) (1.26.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (2025.1.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-vis) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-vis) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-vis) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-vis) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Iterable' from 'collections' (/usr/lib/python3.11/collections/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-15bc6d58a8f8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m \u001b[0;31m# Import matplotlib for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vis/visualization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mactivation_maximization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_activation_with_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mactivation_maximization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vis/visualization/activation_maximization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivationMaximization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTotalVariation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLPNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vis/losses.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vis/utils/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Iterable' from 'collections' (/usr/lib/python3.11/collections/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-vis --upgrade  # Ensure keras-vis is installed and up-to-date\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgakJymknvVQ",
        "outputId": "283a4d5f-1374-4146-bb51-bab37c646afd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-vis in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.12.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from keras-vis) (0.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from keras-vis) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py->keras-vis) (1.26.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (2025.1.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-vis) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-vis) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-vis) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-vis) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}